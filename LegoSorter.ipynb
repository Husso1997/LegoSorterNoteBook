{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import PIL\n",
    "from google.cloud import storage\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "# Google Cloud Storage Properties\n",
    "bucket_name = 'iot-lego-sorter.appspot.com'\n",
    "folder = 'datasets'\n",
    "storage_client = storage.Client.from_service_account_json('service_account.json')\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# Directories\n",
    "localDir = 'images/lego/'\n",
    "\n",
    "# Images\n",
    "imgWidth = 64\n",
    "imgHeight = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List available buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = list(storage_client.list_buckets())\n",
    "print(buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download files from Google Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get available files in datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = storage_client.list_blobs(bucket_name, prefix=folder)\n",
    "fileNameByBrickId = {}\n",
    "\n",
    "for blob in blobs:\n",
    "    nameNoFolder = blob.name[len(folder)+1:]\n",
    "    split = nameNoFolder.split('/')\n",
    "    \n",
    "    if(len(split) > 1):\n",
    "        id = split[0];\n",
    "        fileDir = blob.name;\n",
    "        \n",
    "        if(id in fileNameByBrickId):\n",
    "            fileNameByBrickId[id].append(fileDir)\n",
    "        else:\n",
    "            fileNameByBrickId[id] = [fileDir]\n",
    "print('Retrieved all filenames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metode til at Downloade billeder hvis de ikke allerede er downloaded.\n",
    "def downloadFile(fileName):\n",
    "    nameNoFolder = fileName[len(folder)+1:]\n",
    "    testFileDir = localDir + nameNoFolder\n",
    "    testDir = os.path.dirname(testFileDir)\n",
    "    \n",
    "    if not os.path.isfile(testFileDir):\n",
    "        if not os.path.exists(testDir):\n",
    "            os.makedirs(testDir)\n",
    "        blob = bucket.blob(fileName)\n",
    "        blob.download_to_filename(testFileDir)\n",
    "    \n",
    "    return testFileDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "localFileNameByBrickId = {}\n",
    "for key in fileNameByBrickId.keys():\n",
    "    print(key)\n",
    "    localFileNameByBrickId[key] = []\n",
    "    listLenStr = str(len(fileNameByBrickId[key]))\n",
    "    for index, fileName in enumerate(fileNameByBrickId[key]):\n",
    "        print(str(index+1).zfill(len(listLenStr)) + '/' + listLenStr + ' - ' + fileName + (' ' * 256), end='\\r')\n",
    "        localFileNameByBrickId[key].append(downloadFile(fileName))\n",
    "    print(listLenStr + '/' + listLenStr + ' - Finished downloading' + (' ' * 256))\n",
    "print('\\nFinished all downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make images into multidimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metode til at konvertere billede til multidimensionelt array.\n",
    "def convertImageToArray(fileName):\n",
    "    an_image = PIL.Image.open(fileName).convert('L')\n",
    "    image_sequence = an_image.getdata()\n",
    "    return np.array(image_sequence).reshape(imgWidth, imgHeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndexToBrickId = {}\n",
    "brickIdImageKeyPairs = []\n",
    "# Sammensætter index & billeder\n",
    "for index, key in enumerate(localFileNameByBrickId.keys()):\n",
    "    print(key)\n",
    "    IndexToBrickId[index] = key\n",
    "    listLenStr = str(len(localFileNameByBrickId[key]))\n",
    "    for index2, fileName in enumerate(localFileNameByBrickId[key]):\n",
    "        print(str(index2+1).zfill(len(listLenStr)) + '/' + listLenStr + ' - ' + fileName + (' ' * 256), end='\\r')\n",
    "        brickIdImageKeyPairs.append((index, convertImageToArray(fileName)))\n",
    "    print(listLenStr + '/' + listLenStr + ' - Finished converting\\n')\n",
    "print('\\nFinished converting all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomize order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tilføjer billeder & indexes & shuffle inden\n",
    "IndexesForBrickId = []\n",
    "images = []\n",
    "random.shuffle(brickIdImageKeyPairs)\n",
    "for item in brickIdImageKeyPairs:\n",
    "    IndexesForBrickId.append(item[0])\n",
    "    images.append(item[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting datasets and normalizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tager en procentdel af datasættet til træning og en procentdel til test.\n",
    "percentageToTakeTrainFull = int(len(IndexesForBrickId) * 0.8)\n",
    "\n",
    "# Her skalerer vi pixel intensiteten til mellem 0-1, konverterer til float ved at dividerer med 255\n",
    "images = np.asarray(images) / 255.\n",
    "\n",
    "print('Train ' + str(percentageToTakeTrainFull) + '/' + str(len(IndexesForBrickId)))\n",
    "X_train_full = np.asarray(images[:percentageToTakeTrainFull])\n",
    "y_train_full = np.asarray(IndexesForBrickId[:percentageToTakeTrainFull])\n",
    "\n",
    "X_test = np.asarray(images[percentageToTakeTrainFull:])\n",
    "y_test = np.asarray(IndexesForBrickId[percentageToTakeTrainFull:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Størrelsen på træningssætet & dimensionerne samt datatype\n",
    "print(X_train_full.shape)\n",
    "# Hver pixel intensity er repræsenteret en float mellem 0 - 1\n",
    "print(X_train_full.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentageToTakeForValidation = int(len(X_train_full) * 0.1)\n",
    "print('Length of Validation set ' + str(percentageToTakeForValidation) + ' / ' + str(len(X_train_full)))\n",
    "\n",
    "# Splitter træningssættet til en validation og en lidt mindre træningssæt\n",
    "X_valid, X_train = X_train_full[:percentageToTakeForValidation], X_train_full[percentageToTakeForValidation:]\n",
    "y_valid, y_train = y_train_full[:percentageToTakeForValidation], y_train_full[percentageToTakeForValidation:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Set shape: {0}\".format(X_train.shape))\n",
    "print(\"Test Set shape: {0}\".format(X_test.shape))\n",
    "print(\"Valid Set Shape: {0}\".format(X_valid.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to create model with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(var_activation='relu',var_optimizer='adam', var_neurons='50', var_hiddenL=2, var_lr=0.005, var_momentum = 0.5):\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    # Input layer:\n",
    "    # Flatten layer konverterer hvert input billede til et 1 dimensionelt array.\n",
    "    model.add(keras.layers.Flatten(input_shape=[imgWidth, imgHeight]))\n",
    "\n",
    "    # Hidden layers:\n",
    "    # Tilføjer hidden layers\n",
    "    for h in range(var_hiddenL):\n",
    "        model.add(keras.layers.Dense(var_neurons, activation=var_activation)),\n",
    "\n",
    "    # Output layer.\n",
    "    # Laget skal indeholde en neuron pr. klasse, dvs. i det her tilfælde 3 neurons\n",
    "    # Da det er multiclass classification skal vi bruge softmax activation function\n",
    "    # Det sørger for, at de estimerede sandsynligheder er mellem 0 og 1.\n",
    "    model.add(keras.layers.Dense(3, activation=\"softmax\"))\n",
    "    # Modellen bliver kompileret\n",
    "    if var_optimizer.lower() == 'sgd':\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.SGD(lr=var_lr, momentum=var_momentum),\n",
    "        metrics=[\"accuracy\"])\n",
    "    elif var_optimizer.lower() == 'adam': \n",
    "        \n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=var_lr),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    else:\n",
    "                model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=var_optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RandomizedSearchCV, prøver og at finde de bedste parametrer. \n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "kerasModel = KerasClassifier(build_model, epochs=30)\n",
    "\n",
    "# Vælger de forskellige parameter der skal blive brugt i forbindelse med RandomizedSearchCV\n",
    "_optimizers=['SGD', 'Adam']\n",
    "_neurons=[60, 80, 100]\n",
    "_batch_size=[16,32,64]\n",
    "_activations=['relu','selu']\n",
    "_hiddenLayer=[2, 4]\n",
    "_lr=[0.005, 0.010, 0.0005]\n",
    "_momentum=[0.5, 0.7]\n",
    "params=dict(var_activation=_activations,\n",
    "            var_optimizer=_optimizers,\n",
    "            batch_size=_batch_size,\n",
    "            var_neurons=_neurons,\n",
    "            var_hiddenL=_hiddenLayer,\n",
    "            var_lr=_lr,\n",
    "            var_momentum=_momentum)\n",
    "\n",
    "rscv = RandomizedSearchCV(kerasModel, param_distributions=params, cv=3,n_iter=10)\n",
    "rscv_results = rscv.fit(X_train, y_train, validation_data=(X_valid, y_valid))\n",
    "## Får printet ud de bedste score & parameterere\n",
    "print('Best score is: {} using {}'.format(rscv_results.best_score_,\n",
    "rscv_results.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = build_model('relu', 'sgd', 100, 2, 0.005, 0.5)\n",
    "hidden1_sgd = model_sgd.layers[1]\n",
    "\n",
    "weights_sgd, biases_sgd = hidden1_sgd.get_weights()\n",
    "print(\"Weights:\\n {0}\\n\\nShape of weights {1}\\n\".format(weights_sgd, weights_sgd.shape))\n",
    "print(\"Biases:\\n {0}\\n\\nShape of biases {1}\".format(biases_sgd, biases_sgd.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "# Laver earlyStopping og træner modellen. Earlystopping ruller tilbage til den bedste model\n",
    "earlyStopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model_sgd.fit(X_train, y_train, epochs=30, batch_size=32,\n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    callbacks=[earlyStopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the SGD model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions med de første 3 instanser af test-sættet\n",
    "X_new = X_test[:3]\n",
    "y_pred = np.argmax(model_sgd.predict(X_new), axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ser hvis det er korrekt.\n",
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legoklods billede & ID til legoklods\n",
    "newClassList = np.array(list(IndexToBrickId.values()))[y_pred]\n",
    "for i in range(3):\n",
    "    plt.imshow(X_new[i], cmap='gray')\n",
    "    plt.show()\n",
    "    print(newClassList[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainining the Model (Adam) & evaluating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam = build_model('relu', 'adam', 60, 4, 0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_adam.fit(X_train, y_train, epochs=30, batch_size=16,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
